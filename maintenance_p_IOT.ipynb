{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127040a2-0afd-4302-b235-718777d9bae0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################################################################################\n",
      "PHASE 1 : CHARGEMENT ET ANALYSE EXPLORATOIRE DES DONNÉES\n",
      "################################################################################\n",
      "\n",
      " Chargement des données d'entraînement...\n",
      "Données chargées : 20631 lignes, 26 colonnes\n",
      "Nombre de moteurs : 100\n",
      "\n",
      " Aperçu des données :\n",
      "   unit_nr  time_cycles  op_setting_1  op_setting_2  op_setting_3  sensor_1  \\\n",
      "0        1            1       -0.0007       -0.0004         100.0    518.67   \n",
      "1        1            2        0.0019       -0.0003         100.0    518.67   \n",
      "2        1            3       -0.0043        0.0003         100.0    518.67   \n",
      "3        1            4        0.0007        0.0000         100.0    518.67   \n",
      "4        1            5       -0.0019       -0.0002         100.0    518.67   \n",
      "5        1            6       -0.0043       -0.0001         100.0    518.67   \n",
      "6        1            7        0.0010        0.0001         100.0    518.67   \n",
      "7        1            8       -0.0034        0.0003         100.0    518.67   \n",
      "8        1            9        0.0008        0.0001         100.0    518.67   \n",
      "9        1           10       -0.0033        0.0001         100.0    518.67   \n",
      "\n",
      "   sensor_2  sensor_3  sensor_4  sensor_5  ...  sensor_12  sensor_13  \\\n",
      "0    641.82   1589.70   1400.60     14.62  ...     521.66    2388.02   \n",
      "1    642.15   1591.82   1403.14     14.62  ...     522.28    2388.07   \n",
      "2    642.35   1587.99   1404.20     14.62  ...     522.42    2388.03   \n",
      "3    642.35   1582.79   1401.87     14.62  ...     522.86    2388.08   \n",
      "4    642.37   1582.85   1406.22     14.62  ...     522.19    2388.04   \n",
      "5    642.10   1584.47   1398.37     14.62  ...     521.68    2388.03   \n",
      "6    642.48   1592.32   1397.77     14.62  ...     522.32    2388.03   \n",
      "7    642.56   1582.96   1400.97     14.62  ...     522.47    2388.03   \n",
      "8    642.12   1590.98   1394.80     14.62  ...     521.79    2388.05   \n",
      "9    641.71   1591.24   1400.46     14.62  ...     521.79    2388.06   \n",
      "\n",
      "   sensor_14  sensor_15  sensor_16  sensor_17  sensor_18  sensor_19  \\\n",
      "0    8138.62     8.4195       0.03        392       2388      100.0   \n",
      "1    8131.49     8.4318       0.03        392       2388      100.0   \n",
      "2    8133.23     8.4178       0.03        390       2388      100.0   \n",
      "3    8133.83     8.3682       0.03        392       2388      100.0   \n",
      "4    8133.80     8.4294       0.03        393       2388      100.0   \n",
      "5    8132.85     8.4108       0.03        391       2388      100.0   \n",
      "6    8132.32     8.3974       0.03        392       2388      100.0   \n",
      "7    8131.07     8.4076       0.03        391       2388      100.0   \n",
      "8    8125.69     8.3728       0.03        392       2388      100.0   \n",
      "9    8129.38     8.4286       0.03        393       2388      100.0   \n",
      "\n",
      "   sensor_20  sensor_21  \n",
      "0      39.06    23.4190  \n",
      "1      39.00    23.4236  \n",
      "2      38.95    23.3442  \n",
      "3      38.88    23.3739  \n",
      "4      38.90    23.4044  \n",
      "5      38.98    23.3669  \n",
      "6      39.10    23.3774  \n",
      "7      38.97    23.3106  \n",
      "8      39.05    23.4066  \n",
      "9      38.95    23.4694  \n",
      "\n",
      "[10 rows x 26 columns]\n",
      "\n",
      " Statistiques descriptives :\n",
      "            unit_nr   time_cycles  op_setting_1  op_setting_2  op_setting_3  \\\n",
      "count  20631.000000  20631.000000  20631.000000  20631.000000       20631.0   \n",
      "mean      51.506568    108.807862     -0.000009      0.000002         100.0   \n",
      "std       29.227633     68.880990      0.002187      0.000293           0.0   \n",
      "min        1.000000      1.000000     -0.008700     -0.000600         100.0   \n",
      "25%       26.000000     52.000000     -0.001500     -0.000200         100.0   \n",
      "50%       52.000000    104.000000      0.000000      0.000000         100.0   \n",
      "75%       77.000000    156.000000      0.001500      0.000300         100.0   \n",
      "max      100.000000    362.000000      0.008700      0.000600         100.0   \n",
      "\n",
      "           sensor_1      sensor_2      sensor_3      sensor_4      sensor_5  \\\n",
      "count  2.063100e+04  20631.000000  20631.000000  20631.000000  2.063100e+04   \n",
      "mean   5.186700e+02    642.680934   1590.523119   1408.933782  1.462000e+01   \n",
      "std    6.537152e-11      0.500053      6.131150      9.000605  3.394700e-12   \n",
      "min    5.186700e+02    641.210000   1571.040000   1382.250000  1.462000e+01   \n",
      "25%    5.186700e+02    642.325000   1586.260000   1402.360000  1.462000e+01   \n",
      "50%    5.186700e+02    642.640000   1590.100000   1408.040000  1.462000e+01   \n",
      "75%    5.186700e+02    643.000000   1594.380000   1414.555000  1.462000e+01   \n",
      "max    5.186700e+02    644.530000   1616.910000   1441.490000  1.462000e+01   \n",
      "\n",
      "       ...     sensor_12     sensor_13     sensor_14     sensor_15  \\\n",
      "count  ...  20631.000000  20631.000000  20631.000000  20631.000000   \n",
      "mean   ...    521.413470   2388.096152   8143.752722      8.442146   \n",
      "std    ...      0.737553      0.071919     19.076176      0.037505   \n",
      "min    ...    518.690000   2387.880000   8099.940000      8.324900   \n",
      "25%    ...    520.960000   2388.040000   8133.245000      8.414900   \n",
      "50%    ...    521.480000   2388.090000   8140.540000      8.438900   \n",
      "75%    ...    521.950000   2388.140000   8148.310000      8.465600   \n",
      "max    ...    523.380000   2388.560000   8293.720000      8.584800   \n",
      "\n",
      "          sensor_16     sensor_17  sensor_18  sensor_19     sensor_20  \\\n",
      "count  2.063100e+04  20631.000000    20631.0    20631.0  20631.000000   \n",
      "mean   3.000000e-02    393.210654     2388.0      100.0     38.816271   \n",
      "std    1.556432e-14      1.548763        0.0        0.0      0.180746   \n",
      "min    3.000000e-02    388.000000     2388.0      100.0     38.140000   \n",
      "25%    3.000000e-02    392.000000     2388.0      100.0     38.700000   \n",
      "50%    3.000000e-02    393.000000     2388.0      100.0     38.830000   \n",
      "75%    3.000000e-02    394.000000     2388.0      100.0     38.950000   \n",
      "max    3.000000e-02    400.000000     2388.0      100.0     39.430000   \n",
      "\n",
      "          sensor_21  \n",
      "count  20631.000000  \n",
      "mean      23.289705  \n",
      "std        0.108251  \n",
      "min       22.894200  \n",
      "25%       23.221800  \n",
      "50%       23.297900  \n",
      "75%       23.366800  \n",
      "max       23.618400  \n",
      "\n",
      "[8 rows x 26 columns]\n",
      "\n",
      " Analyse de la durée de vie des moteurs...\n",
      " Graphique sauvegardé : duree_vie_moteurs.png\n",
      "\n",
      " Analyse de l'évolution des capteurs pour le moteur #1...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuration des graphiques\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "#################################################\n",
    "# PHASE 1 : CHARGEMENT ET ANALYSE EXPLORATOIRE DES DONNÉES (EDA)\n",
    "#################################################\n",
    "\n",
    "print(\"#\" * 80)\n",
    "print(\"PHASE 1 : CHARGEMENT ET ANALYSE EXPLORATOIRE DES DONNÉES\")\n",
    "print(\"#\" * 80)\n",
    "\n",
    "\n",
    "# Définition des noms de colonnes (car les fichiers n'ont pas d'en-tête)\n",
    "column_names = ['unit_nr', 'time_cycles']\n",
    "column_names += ['op_setting_' + str(i) for i in range(1, 4)]\n",
    "column_names += ['sensor_' + str(i) for i in range(1, 22)]\n",
    "\n",
    "# Chargement des données d'entraînement\n",
    "print(\"\\n Chargement des données d'entraînement...\")\n",
    "\n",
    "train_data = pd.read_csv('train_FD001.txt', sep='\\s+', header=None, names=column_names)\n",
    "\n",
    "print(f\"Données chargées : {train_data.shape[0]} lignes, {train_data.shape[1]} colonnes\")\n",
    "print(f\"Nombre de moteurs : {train_data['unit_nr'].nunique()}\")\n",
    "\n",
    "# Affichage des premières lignes\n",
    "print(\"\\n Aperçu des données :\")\n",
    "print(train_data.head(10))\n",
    "\n",
    "# Statistiques descriptives\n",
    "print(\"\\n Statistiques descriptives :\")\n",
    "print(train_data.describe())\n",
    "\n",
    "# Visualisation 1 : Évolution des cycles par moteur\n",
    "print(\"\\n Analyse de la durée de vie des moteurs...\")\n",
    "max_cycles = train_data.groupby('unit_nr')['time_cycles'].max()\n",
    "plt.figure(figsize=(14, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(max_cycles, bins=30, edgecolor='black', alpha=0.7)\n",
    "plt.xlabel('Nombre de cycles jusqu\\'à la panne')\n",
    "plt.ylabel('Nombre de moteurs')\n",
    "plt.title('Distribution de la durée de vie des moteurs')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(max_cycles.values, 'o-', alpha=0.6)\n",
    "plt.xlabel('ID Moteur')\n",
    "plt.ylabel('Cycles jusqu\\'à la panne')\n",
    "plt.title('Durée de vie par moteur')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('duree_vie_moteurs.png', dpi=300, bbox_inches='tight')\n",
    "print(\" Graphique sauvegardé : duree_vie_moteurs.png\")\n",
    "\n",
    "# Visualisation 2 : Évolution des capteurs pour un moteur spécifique\n",
    "print(\"\\n Analyse de l'évolution des capteurs pour le moteur #1...\")\n",
    "moteur_1 = train_data[train_data['unit_nr'] == 1]\n",
    "sensor_cols = [col for col in train_data.columns if 'sensor_' in col]\n",
    "\n",
    "fig, axes = plt.subplots(7, 3, figsize=(18, 20))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, sensor in enumerate(sensor_cols):\n",
    "    axes[idx].plot(moteur_1['time_cycles'], moteur_1[sensor], linewidth=2)\n",
    "    axes[idx].set_title(f'{sensor}', fontsize=10, fontweight='bold')\n",
    "    axes[idx].set_xlabel('Cycles')\n",
    "    axes[idx].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('evolution_capteurs_moteur1.png', dpi=300, bbox_inches='tight')\n",
    "print(\"Graphique sauvegardé : evolution_capteurs_moteur1.png\")\n",
    "\n",
    "# Identification des capteurs constants (inutiles pour la prédiction)\n",
    "print(\"\\n Identification des capteurs constants...\")\n",
    "constant_sensors = []\n",
    "for sensor in sensor_cols:\n",
    "    if train_data[sensor].std() < 0.01:  # Écart-type très faible = capteur constant\n",
    "        constant_sensors.append(sensor)\n",
    "        print(f\"   {sensor} est constant (std = {train_data[sensor].std():.6f})\")\n",
    "\n",
    "print(f\"\\n {len(constant_sensors)} capteurs constants identifiés\")\n",
    "\n",
    "# Matrice de corrélation\n",
    "print(\"\\n Calcul de la matrice de corrélation...\")\n",
    "correlation_matrix = train_data[sensor_cols].corr()\n",
    "plt.figure(figsize=(16, 14))\n",
    "sns.heatmap(correlation_matrix, annot=False, cmap='coolwarm', center=0, \n",
    "            square=True, linewidths=0.5, cbar_kws={\"shrink\": 0.8})\n",
    "plt.title('Matrice de Corrélation des Capteurs', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig('matrice_correlation.png', dpi=300, bbox_inches='tight')\n",
    "print(\" Graphique sauvegardé : matrice_correlation.png\")\n",
    "\n",
    "#################################################\n",
    "# PHASE 2 : PRÉTRAITEMENT ET FEATURE ENGINEERING\n",
    "#################################################\n",
    "\n",
    "print(\"\\n\" + \"#\" * 80)\n",
    "print(\"PHASE 2 : PRÉTRAITEMENT ET CALCUL DU RUL\")\n",
    "print(\"#\" * 80)\n",
    "\n",
    "# CALCUL DU RUL (Remaining Useful Life)\n",
    "print(\"\\n  Calcul du RUL pour chaque cycle...\")\n",
    "\n",
    "# Pour chaque moteur, on trouve son cycle maximal (moment de la panne)\n",
    "max_cycles_per_engine = train_data.groupby('unit_nr')['time_cycles'].max().reset_index()\n",
    "max_cycles_per_engine.columns = ['unit_nr', 'max_cycle']\n",
    "\n",
    "# On fusionne avec les données d'origine\n",
    "train_data = train_data.merge(max_cycles_per_engine, on='unit_nr', how='left')\n",
    "\n",
    "# Calcul du RUL : RUL = Cycle_Max - Cycle_Actuel\n",
    "train_data['RUL'] = train_data['max_cycle'] - train_data['time_cycles']\n",
    "\n",
    "print(\" Colonne RUL créée avec succès\")\n",
    "print(f\"\\n Exemple de RUL pour le moteur #1 :\")\n",
    "print(train_data[train_data['unit_nr'] == 1][['unit_nr', 'time_cycles', 'max_cycle', 'RUL']].head(10))\n",
    "\n",
    "# Visualisation du RUL\n",
    "plt.figure(figsize=(14, 6))\n",
    "for i in range(1, 6):  # Afficher 5 premiers moteurs\n",
    "    engine_data = train_data[train_data['unit_nr'] == i]\n",
    "    plt.plot(engine_data['time_cycles'], engine_data['RUL'], \n",
    "             label=f'Moteur {i}', linewidth=2, alpha=0.7)\n",
    "\n",
    "plt.xlabel('Cycles', fontsize=12)\n",
    "plt.ylabel('RUL (Remaining Useful Life)', fontsize=12)\n",
    "plt.title('Évolution du RUL pour les 5 premiers moteurs', fontsize=14, fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('evolution_rul.png', dpi=300, bbox_inches='tight')\n",
    "print(\" Graphique sauvegardé : evolution_rul.png\")\n",
    "\n",
    "# NETTOYAGE DES DONNÉES\n",
    "print(\"\\n Nettoyage des données...\")\n",
    "\n",
    "# Suppression des capteurs constants\n",
    "print(f\"   Suppression de {len(constant_sensors)} capteurs constants\")\n",
    "train_cleaned = train_data.drop(columns=constant_sensors)\n",
    "\n",
    "# Suppression de la colonne max_cycle (temporaire, plus nécessaire)\n",
    "train_cleaned = train_cleaned.drop(columns=['max_cycle'])\n",
    "\n",
    "print(f\"Dataset nettoyé : {train_cleaned.shape[1]} colonnes restantes\")\n",
    "\n",
    "# NORMALISATION DES DONNÉES\n",
    "print(\"\\n Normalisation des données (MinMaxScaler)...\")\n",
    "\n",
    "# Colonnes à normaliser (tous les capteurs et paramètres opérationnels)\n",
    "cols_to_normalize = [col for col in train_cleaned.columns \n",
    "                     if col not in ['unit_nr', 'time_cycles', 'RUL']]\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "train_cleaned[cols_to_normalize] = scaler.fit_transform(train_cleaned[cols_to_normalize])\n",
    "\n",
    "print(\" Normalisation terminée (valeurs entre 0 et 1)\")\n",
    "print(f\"\\n Aperçu des données normalisées :\")\n",
    "print(train_cleaned.head())\n",
    "\n",
    "##################################################\n",
    "# PHASE 3 : MODÉLISATION (MACHINE LEARNING)\n",
    "##################################################\n",
    "\n",
    "print(\"\\n\" + \"#\" * 80)\n",
    "print(\"PHASE 3 : MODÉLISATION ET ENTRAÎNEMENT\")\n",
    "print(\"#\" * 80)\n",
    "\n",
    "# Préparation des données pour l'entraînement\n",
    "print(\"\\n Préparation des features (X) et de la cible (y)...\")\n",
    "\n",
    "# X = Features (on enlève unit_nr, time_cycles, et RUL)\n",
    "X = train_cleaned.drop(columns=['unit_nr', 'time_cycles', 'RUL'])\n",
    "# y = Target (le RUL qu'on veut prédire)\n",
    "y = train_cleaned['RUL']\n",
    "\n",
    "print(f\" Features (X) : {X.shape[1]} colonnes, {X.shape[0]} échantillons\")\n",
    "print(f\" Target (y) : {y.shape[0]} échantillons\")\n",
    "\n",
    "# Division Train/Test (80/20)\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"\\n Données d'entraînement : {X_train.shape[0]} échantillons\")\n",
    "print(f\" Données de test : {X_test.shape[0]} échantillons\")\n",
    "\n",
    "##################################################\n",
    "# MODÈLE 1 : RÉGRESSION LINÉAIRE (BASELINE)\n",
    "##################################################\n",
    "\n",
    "print(\"\\n\" + \"-\" * 60)\n",
    "print(\"MODÈLE 1 : RÉGRESSION LINÉAIRE (Baseline)\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "print(\"\\n Entraînement du modèle de régression linéaire...\")\n",
    "model_lr = LinearRegression()\n",
    "model_lr.fit(X_train, y_train)\n",
    "print(\"Entraînement terminé\")\n",
    "\n",
    "# Prédictions\n",
    "y_pred_lr = model_lr.predict(X_test)\n",
    "\n",
    "# Évaluation\n",
    "rmse_lr = np.sqrt(mean_squared_error(y_test, y_pred_lr))\n",
    "mae_lr = mean_absolute_error(y_test, y_pred_lr)\n",
    "r2_lr = r2_score(y_test, y_pred_lr)\n",
    "\n",
    "print(f\"\\n RÉSULTATS - Régression Linéaire :\")\n",
    "print(f\"  • RMSE (Root Mean Square Error) : {rmse_lr:.2f} cycles\")\n",
    "print(f\"  • MAE (Mean Absolute Error)      : {mae_lr:.2f} cycles\")\n",
    "print(f\"  • R² Score                       : {r2_lr:.4f}\")\n",
    "\n",
    "##################################################\n",
    "# MODÈLE 2 : RANDOM FOREST (AVANCÉ)\n",
    "##################################################\n",
    "\n",
    "print(\"\\n\" + \"-\" * 60)\n",
    "print(\"MODÈLE 2 : RANDOM FOREST REGRESSOR (Avancé)\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "print(\"\\n Entraînement du Random Forest (100 arbres)...\")\n",
    "model_rf = RandomForestRegressor(n_estimators=100, max_depth=20, \n",
    "                                 random_state=42, n_jobs=-1, verbose=0)\n",
    "model_rf.fit(X_train, y_train)\n",
    "print(\" Entraînement terminé\")\n",
    "\n",
    "# Prédictions\n",
    "y_pred_rf = model_rf.predict(X_test)\n",
    "\n",
    "# Évaluation\n",
    "rmse_rf = np.sqrt(mean_squared_error(y_test, y_pred_rf))\n",
    "mae_rf = mean_absolute_error(y_test, y_pred_rf)\n",
    "r2_rf = r2_score(y_test, y_pred_rf)\n",
    "\n",
    "print(f\"\\n RÉSULTATS - Random Forest :\")\n",
    "print(f\"  • RMSE (Root Mean Square Error) : {rmse_rf:.2f} cycles\")\n",
    "print(f\"  • MAE (Mean Absolute Error)      : {mae_rf:.2f} cycles\")\n",
    "print(f\"  • R² Score                       : {r2_rf:.4f}\")\n",
    "\n",
    "# Importance des features\n",
    "print(\"\\n Top 10 des capteurs les plus importants (Random Forest) :\")\n",
    "feature_importance_rf = pd.DataFrame({\n",
    "    'feature': X.columns,\n",
    "    'importance': model_rf.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(feature_importance_rf.head(10).to_string(index=False))\n",
    "\n",
    "##################################################\n",
    "# MODÈLE 3 : XGBOOST (TRÈS PERFORMANT)\n",
    "##################################################\n",
    "\n",
    "print(\"\\n\" + \"-\" * 60)\n",
    "print(\"MODÈLE 3 : XGBOOST REGRESSOR (État de l'art)\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "try:\n",
    "    import xgboost as xgb\n",
    "    \n",
    "    print(\"\\n Entraînement du XGBoost...\")\n",
    "    model_xgb = xgb.XGBRegressor(\n",
    "        n_estimators=200,           # Nombre d'arbres\n",
    "        max_depth=8,                # Profondeur maximale\n",
    "        learning_rate=0.1,          # Taux d'apprentissage\n",
    "        subsample=0.8,              # Échantillonnage des données\n",
    "        colsample_bytree=0.8,       # Échantillonnage des features\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        verbosity=0\n",
    "    )\n",
    "    \n",
    "    model_xgb.fit(X_train, y_train, \n",
    "                  eval_set=[(X_test, y_test)],\n",
    "                  verbose=False)\n",
    "    print(\" Entraînement terminé\")\n",
    "    \n",
    "    # Prédictions\n",
    "    y_pred_xgb = model_xgb.predict(X_test)\n",
    "    \n",
    "    # Évaluation\n",
    "    rmse_xgb = np.sqrt(mean_squared_error(y_test, y_pred_xgb))\n",
    "    mae_xgb = mean_absolute_error(y_test, y_pred_xgb)\n",
    "    r2_xgb = r2_score(y_test, y_pred_xgb)\n",
    "    \n",
    "    print(f\"\\n RÉSULTATS - XGBoost :\")\n",
    "    print(f\"  • RMSE (Root Mean Square Error) : {rmse_xgb:.2f} cycles\")\n",
    "    print(f\"  • MAE (Mean Absolute Error)      : {mae_xgb:.2f} cycles\")\n",
    "    print(f\"  • R² Score                       : {r2_xgb:.4f}\")\n",
    "    \n",
    "    # Importance des features XGBoost\n",
    "    print(\"\\n Top 10 des capteurs les plus importants (XGBoost) :\")\n",
    "    feature_importance_xgb = pd.DataFrame({\n",
    "        'feature': X.columns,\n",
    "        'importance': model_xgb.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    print(feature_importance_xgb.head(10).to_string(index=False))\n",
    "    \n",
    "    # Visualisation de l'importance des features (XGBoost)\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    top_features = feature_importance_xgb.head(15)\n",
    "    plt.barh(range(len(top_features)), top_features['importance'])\n",
    "    plt.yticks(range(len(top_features)), top_features['feature'])\n",
    "    plt.xlabel('Importance', fontsize=12)\n",
    "    plt.ylabel('Capteurs', fontsize=12)\n",
    "    plt.title('Top 15 des Features les Plus Importantes (XGBoost)', \n",
    "              fontsize=14, fontweight='bold')\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.grid(True, alpha=0.3, axis='x')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('feature_importance_xgboost.png', dpi=300, bbox_inches='tight')\n",
    "    print(\"\\n Graphique sauvegardé : feature_importance_xgboost.png\")\n",
    "    \n",
    "    xgb_available = True\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"\\n  XGBoost n'est pas installé.\")\n",
    "    print(\"   Pour l'installer : pip install xgboost\")\n",
    "    print(\"   Le modèle XGBoost sera ignoré dans l'analyse.\")\n",
    "    xgb_available = False\n",
    "    rmse_xgb, mae_xgb, r2_xgb = None, None, None\n",
    "    y_pred_xgb = None\n",
    "\n",
    "##################################################\n",
    "# PHASE 4 : VISUALISATION DES RÉSULTATS\n",
    "##################################################\n",
    "\n",
    "print(\"\\n\" + \"#\" * 80)\n",
    "print(\"PHASE 4 : VISUALISATION DES RÉSULTATS\")\n",
    "print(\"#\" * 80)\n",
    "\n",
    "# Comparaison des modèles\n",
    "if xgb_available:\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(20, 6))\n",
    "    \n",
    "    # Régression Linéaire\n",
    "    axes[0].scatter(y_test, y_pred_lr, alpha=0.5, s=10)\n",
    "    axes[0].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], \n",
    "                 'r--', linewidth=2, label='Prédiction parfaite')\n",
    "    axes[0].set_xlabel('RUL Réel (cycles)', fontsize=12)\n",
    "    axes[0].set_ylabel('RUL Prédit (cycles)', fontsize=12)\n",
    "    axes[0].set_title(f'Régression Linéaire\\nRMSE = {rmse_lr:.2f}', \n",
    "                      fontsize=14, fontweight='bold')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Random Forest\n",
    "    axes[1].scatter(y_test, y_pred_rf, alpha=0.5, s=10, color='green')\n",
    "    axes[1].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], \n",
    "                 'r--', linewidth=2, label='Prédiction parfaite')\n",
    "    axes[1].set_xlabel('RUL Réel (cycles)', fontsize=12)\n",
    "    axes[1].set_ylabel('RUL Prédit (cycles)', fontsize=12)\n",
    "    axes[1].set_title(f'Random Forest\\nRMSE = {rmse_rf:.2f}', \n",
    "                      fontsize=14, fontweight='bold')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # XGBoost\n",
    "    axes[2].scatter(y_test, y_pred_xgb, alpha=0.5, s=10, color='orange')\n",
    "    axes[2].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], \n",
    "                 'r--', linewidth=2, label='Prédiction parfaite')\n",
    "    axes[2].set_xlabel('RUL Réel (cycles)', fontsize=12)\n",
    "    axes[2].set_ylabel('RUL Prédit (cycles)', fontsize=12)\n",
    "    axes[2].set_title(f'XGBoost\\nRMSE = {rmse_xgb:.2f}', \n",
    "                      fontsize=14, fontweight='bold')\n",
    "    axes[2].legend()\n",
    "    axes[2].grid(True, alpha=0.3)\n",
    "    \n",
    "else:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "    # Régression Linéaire\n",
    "    axes[0].scatter(y_test, y_pred_lr, alpha=0.5, s=10)\n",
    "    axes[0].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], \n",
    "                 'r--', linewidth=2, label='Prédiction parfaite')\n",
    "    axes[0].set_xlabel('RUL Réel (cycles)', fontsize=12)\n",
    "    axes[0].set_ylabel('RUL Prédit (cycles)', fontsize=12)\n",
    "    axes[0].set_title(f'Régression Linéaire\\nRMSE = {rmse_lr:.2f}', \n",
    "                      fontsize=14, fontweight='bold')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Random Forest\n",
    "    axes[1].scatter(y_test, y_pred_rf, alpha=0.5, s=10, color='green')\n",
    "    axes[1].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], \n",
    "                 'r--', linewidth=2, label='Prédiction parfaite')\n",
    "    axes[1].set_xlabel('RUL Réel (cycles)', fontsize=12)\n",
    "    axes[1].set_ylabel('RUL Prédit (cycles)', fontsize=12)\n",
    "    axes[1].set_title(f'Random Forest\\nRMSE = {rmse_rf:.2f}', \n",
    "                      fontsize=14, fontweight='bold')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('comparaison_modeles.png', dpi=300, bbox_inches='tight')\n",
    "print(\"\\n Graphique sauvegardé : comparaison_modeles.png\")\n",
    "\n",
    "# Tableau comparatif final\n",
    "print(\"\\n\" + \"#\" * 80)\n",
    "print(\"TABLEAU COMPARATIF FINAL\")\n",
    "print(\"#\" * 80)\n",
    "\n",
    "if xgb_available:\n",
    "    comparison_df = pd.DataFrame({\n",
    "        'Modèle': ['Régression Linéaire', 'Random Forest', 'XGBoost'],\n",
    "        'RMSE': [f'{rmse_lr:.2f}', f'{rmse_rf:.2f}', f'{rmse_xgb:.2f}'],\n",
    "        'MAE': [f'{mae_lr:.2f}', f'{mae_rf:.2f}', f'{mae_xgb:.2f}'],\n",
    "        'R² Score': [f'{r2_lr:.4f}', f'{r2_rf:.4f}', f'{r2_xgb:.4f}']\n",
    "    })\n",
    "    \n",
    "    # Déterminer le meilleur modèle\n",
    "    best_model_idx = [rmse_lr, rmse_rf, rmse_xgb].index(min([rmse_lr, rmse_rf, rmse_xgb]))\n",
    "    best_model_name = ['Régression Linéaire', 'Random Forest', 'XGBoost'][best_model_idx]\n",
    "    best_rmse = min([rmse_lr, rmse_rf, rmse_xgb])\n",
    "    \n",
    "else:\n",
    "    comparison_df = pd.DataFrame({\n",
    "        'Modèle': ['Régression Linéaire', 'Random Forest'],\n",
    "        'RMSE': [f'{rmse_lr:.2f}', f'{rmse_rf:.2f}'],\n",
    "        'MAE': [f'{mae_lr:.2f}', f'{mae_rf:.2f}'],\n",
    "        'R² Score': [f'{r2_lr:.4f}', f'{r2_rf:.4f}']\n",
    "    })\n",
    "    \n",
    "    # Déterminer le meilleur modèle\n",
    "    best_model_idx = [rmse_lr, rmse_rf].index(min([rmse_lr, rmse_rf]))\n",
    "    best_model_name = ['Régression Linéaire', 'Random Forest'][best_model_idx]\n",
    "    best_rmse = min([rmse_lr, rmse_rf])\n",
    "\n",
    "print(\"\\n\" + comparison_df.to_string(index=False))\n",
    "\n",
    "# Graphique comparatif des performances\n",
    "plt.figure(figsize=(12, 6))\n",
    "models = comparison_df['Modèle'].tolist()\n",
    "rmse_values = [float(x) for x in comparison_df['RMSE'].tolist()]\n",
    "colors = ['#3498db', '#2ecc71', '#e74c3c'][:len(models)]\n",
    "\n",
    "bars = plt.bar(models, rmse_values, color=colors, alpha=0.7, edgecolor='black', linewidth=2)\n",
    "plt.ylabel('RMSE (cycles)', fontsize=14, fontweight='bold')\n",
    "plt.title('Comparaison des Performances des Modèles', fontsize=16, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Ajouter les valeurs sur les barres\n",
    "for bar, value in zip(bars, rmse_values):\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2., height,\n",
    "             f'{value:.2f}',\n",
    "             ha='center', va='bottom', fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('comparaison_rmse.png', dpi=300, bbox_inches='tight')\n",
    "print(\"\\n Graphique sauvegardé : comparaison_rmse.png\")\n",
    "\n",
    "# Recommandation\n",
    "print(\"\\n\" + \"#\" * 80)\n",
    "print(\"CONCLUSION ET RECOMMANDATION\")\n",
    "print(\"#\" * 80)\n",
    "\n",
    "print(f\"\\n MEILLEUR MODÈLE : {best_model_name}\")\n",
    "print(f\"   • RMSE = {best_rmse:.2f} cycles\")\n",
    "\n",
    "if xgb_available and best_model_name == 'XGBoost':\n",
    "    improvement_vs_lr = ((rmse_lr - rmse_xgb) / rmse_lr * 100)\n",
    "    improvement_vs_rf = ((rmse_rf - rmse_xgb) / rmse_rf * 100)\n",
    "    print(f\"   • Amélioration vs Régression Linéaire : {improvement_vs_lr:.1f}%\")\n",
    "    print(f\"   • Amélioration vs Random Forest : {improvement_vs_rf:.1f}%\")\n",
    "elif best_model_name == 'Random Forest':\n",
    "    improvement_vs_lr = ((rmse_lr - rmse_rf) / rmse_lr * 100)\n",
    "    print(f\"   • Amélioration vs Régression Linéaire : {improvement_vs_lr:.1f}%\")\n",
    "    \n",
    "print(\"\\n Recommandation : Utiliser \" + best_model_name + \" pour la prédiction du RUL\")\n",
    "    \n",
    "print(\"\\n  Pour une utilisation en production :\")\n",
    "print(\"  • Tester sur le fichier test_FD001.txt avec RUL_FD001.txt\")\n",
    "print(\"  • Viser un RMSE < 20 cycles pour une fiabilité acceptable\")\n",
    "print(\"  • Implémenter un système d'alertes pour RUL < seuil critique\")\n",
    "print(\"  • Considérer des modèles Deep Learning (LSTM) pour capturer les séquences temporelles\")\n",
    "\n",
    "print(\"\\n ANALYSE DES PERFORMANCES :\")\n",
    "if best_rmse < 20:\n",
    "    print(\"   Excellentes performances (RMSE < 20) - Prêt pour des tests pilotes\")\n",
    "elif best_rmse < 30:\n",
    "    print(\"   Performances correctes (20 < RMSE < 30) - Amélioration recommandée\")\n",
    "else:\n",
    "    print(\"   Performances insuffisantes (RMSE > 30) - Nécessite optimisation\")\n",
    "\n",
    "print(\"\\n\" + \"#\" * 80)\n",
    "print(\"FIN DE L'ANALYSE\")\n",
    "print(\"#\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e71adab-4d77-4bbc-aafe-477e258b88a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
